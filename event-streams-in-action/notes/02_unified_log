-----------------------------------------------------------------------
| CHAPTER 2 - THE UNIFIED LOG                                         |
-----------------------------------------------------------------------

- A Unified Log

    - A 'unified log' is an append-only, ordered, distributed log that allows a company to centralize its
        continuous event streams.  


    - Unified

        - Kafka is designed to allow a single cluster to serve as the central data backbone for a large
            organization.

        - The single log can contain many distinct streams of events.  For instance, a Taxi company has
            a Bookings stream, a Monitoring stream, and a Dispatch stream.


    - Append-Only

        - New events are appended to the front of the unified log, but existing events are never updated in
            place after they're appended.

        - Events are automatically deleted from the unified log when they age beyond a configured time
            window, but the cannot be deleted in an ad hoc fashion.

        - This makes reading much easier, since you only have to read events 1-10 once, for instance.

        - If you make a mistake, you can't just go and correct it.  So, we need to carefully model our
            events.


    - Distributed

        - The unified log is distributed, because it lives across a cluster of machines.

        - This means the log is scalable and durable.

        - To make it easier to scale across machines, we divide the events in a given event stream into
            shards (aka partitions).  Each shard will be replicated to multiple machines for durability,
            but one machine will be elected leader to handle all reads and writes.


    - Ordered

        - The unified log gives each event in a shard a sequential ID number (called an offset) that
            uniquely identifies each message within the shard.

        - This way, different applications can each maintain their own cursor position for each shard,
            telling them which events they have already processed, and which events they should process
            next.



- Introducing Our Application

    - In each part of this book, we'll be working with a company that wants to implement a unified log
        across it's business.  Our first company will be an e-commerce company called 'Nile'.


    - Online shoppers browse products on the Nile website, sometimes adding products to their shopping cart,
        and sometimes then going on to buy those products through the online checkout.


    - We can already see 3 discrete events in this Viewing through Buying workflow:

        1. Shopper views product at time

             This occurs every time a shopper views a product, whether on the product's Detail page or on
               a general Catalog page that happens to include the product.

        2. Shopper adds product to cart at time

             This occurs whenever a shopper adds one of those products to the cart.  A quantity of one or
                more is attached.

        3. Shopper places order at time

             This occurs when the shopper checks out, paying for the items in the shopping cart.



- Unified Log, E-Commerce Style

    - We can define an initial event stream (topic) to record the events generated by shoppers.  We'll call
        this topic 'raw-events'.


    - We have 2 ways that events are captured:

        - Browser-generated events

            The 'shopper views product' and 'shopper adds item to cart' events occur in the user's browser.
              Typically, there would be some JS code to send the events to an HTTP-based event collector,
              which would in turn be tasked with writing the event to the unified log.

        - Server-side events

            A valid 'shopper places order' event is confirmed server-side only after the payment processing
              has completed.  It is the responsibility of the web server to write this event to Kafka.


    - Unified logs don't typically have an opinion about the structure of events.  It is up to us to model
        our events.



- Modelling Our First Event

    - The first event we will model is 'Shopper views product at time'.  

        Subject: Shopper
        Verb: Views
        Direct object: Product
        Prepositional object: at time


    - To start with, we'll model our event in JSON:

        {
          "event": "SHOPPER_VIEWED_PRODUCT",
          "shopper": {
            "id": "123",
            "name": "Jane",
            "ipAddress": "70.46.123.145"
          },
          "product": {
            "sku": "aapl-001",
            "name": "iPad"
          },
          "timestamp": "2018-10-15T12:01:35Z"
        }



- Setting Up the Unified Log

    - First, we'll download and install Kafka and ZooKeeper.


    - Now, we'll create a topic called 'raw-events'.

        # Create new topic
        $ bin/kafka-topics.sh --create --topic raw-events \
            --zookeeper localhost:2181 --replication-factor 1 --partitions 1

        # View topics
        $ bin/kafka-topics.sh --list --zookeeper localhost:2181


    - Start a producer thread and add a few events:

        $ bin/kafka-console-producer.sh --topic raw-events --broker-list localhost:9092

        { "event": "SHOPPER_VIEWED_PRODUCT", "shopper": { "id": "123", "name":
          "Jane", "ipAddress": "70.46.123.145" }, "product": { "sku": "aapl-001",
           "name": "iPad" }, "timestamp": "2018-10-15T12:01:35Z" }

        { "event": "SHOPPER_VIEWED_PRODUCT", "shopper": { "id": "456", "name":
          Mo", "ipAddress": "89.92.213.32" }, "product": { "sku": "sony-072", "name":
          "Widescreen TV" }, "timestamp": "2018-10-15T12:03:45Z" }

        { "event": "SHOPPER_VIEWED_PRODUCT", "shopper": { "id": "789", "name":
          "Justin", "ipAddress": "97.107.137.164" }, "product": { "sku": "ms-003",
          "name": "XBox One" }, "timestamp": "2018-10-15T12:05:05Z" }


    - Now, we'll start a consumer thread, which will read the events:

        $ bin/kafka-console-consumer.sh --topic raw-events --from-beginning \
            --bootstrap-server localhost:9092


    - Now, we can start another consumer thread again with the exact same command, and we'll see the same
        messages.